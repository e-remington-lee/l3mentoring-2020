{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "c5161659e0c4ba7248a1564d775f97f6b0cb1faea6f43745a651b61be4b2f423"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import bert\n",
    "import tqdm\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "from transformers import BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"intent/train.csv\")\n",
    "validation = pd.read_csv(\"intent/valid.csv\")\n",
    "test = pd.read_csv(\"intent/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(train.intent, palette=HAPPY_COLORS_PALETTE)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in train.iterrows():\n",
    "    print(row[\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDetection:\n",
    "    DATA = \"text\"\n",
    "    LABELS = \"intent\"\n",
    "    def __init__(self, train, test, classes, tokenizer:FullTokenizer, absolute_max=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classes = classes\n",
    "        self.absolute_max = absolute_max\n",
    "        self.max_sequence_length = 0\n",
    "        self.attention_mask = []\n",
    "\n",
    "        x, y = map(self._get_max_length, [train, test])\n",
    "        if self.max_sequence_length > self.absolute_max:\n",
    "            print(\"### overriding calculated max sequence length\")\n",
    "            self.max_sequence_length = self.absolute_max\n",
    "\n",
    "        # ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        self.test_x, self.test_y, self.test_original = self._prepare(test)\n",
    "        # self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _get_max_length(self, data_frame):\n",
    "        for _, row in data_frame.iterrows():\n",
    "            sequence = row[\"text\"]\n",
    "            self.max_sequence_length = max(len(sequence), self.max_sequence_length)\n",
    "\n",
    "    def _prepare(self, data_frame):\n",
    "        x, y, z = [], [], []\n",
    "        count = 0\n",
    "        for _, row in tqdm.tqdm(data_frame.iterrows()):\n",
    "            text, label = row[IntentDetection.DATA], row[IntentDetection.LABELS]\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = self.max_sequence_length,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'tf',     # Return tensorflow tensors.\n",
    "                        truncation=True # truncates to max sequence length\n",
    "                   )\n",
    "            x.append(encoded_dict[\"input_ids\"])\n",
    "            z.append(text)\n",
    "            self.attention_mask.append(encoded_dict[\"attention_mask\"])\n",
    "            y.append(self.classes.index(label))\n",
    "            # print(f\"count:{count}\")\n",
    "            count +=1\n",
    "        return np.array(x), np.array(y), z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = IntentDetection(train, test, train.intent.unique().tolist(), tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dict = tokenizer.encode_plus(\n",
    "                        'add sabrina salerno to the grime instrumentals playlist',                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 32,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'tf',     # Return tensorflow tensors.\n",
    "                        truncation=True # truncates to max sequence length\n",
    "                   )\n",
    "print(encoded_dict[\"input_ids\"])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}