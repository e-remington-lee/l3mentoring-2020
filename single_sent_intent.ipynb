{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('venv')",
   "display_name": "Python 3.7.7 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "8710ffc08e7bc82dafd6607f726ddc6e83fa74f0a6cb599995b2b332ab9b1ebc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import bert\n",
    "import tqdm\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "# https://www.youtube.com/watch?v=gE-95nFF4Cc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 1OlcvGWReJMuyYQuOZm149vHWwPtlboR6 --output intent/train.csv\n",
    "# !gdown --id 1Oi5cRlTybuIF2Fl5Bfsr-KkqrXrdt77w --output intent/valid.csv\n",
    "# !gdown --id 1ep9H6-HvhB4utJRLVcLzieWNUSG3P_uF --output intent/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"intent/train.csv\")\n",
    "validation = pd.read_csv(\"intent/valid.csv\")\n",
    "test = pd.read_csv(\"intent/test.csv\")\n",
    "\n",
    "\n",
    "train=train.append(validation).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(train.intent, palette=HAPPY_COLORS_PALETTE)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles the tokenizing and padding of our dataset. Each dataset will have a slightly different way to do each step. On regular\n",
    "# TF packages, https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/2Cyzs/padding, you can see that they have methods that # tokenize and pad together that work seamlessly. Because we are using a pretrained model, bert-tf2, these are not available to us (not sure why tbh, probably because we want to use the pretrained tokenizer which requires us to not use TF methods)\n",
    "class IntentDetection:\n",
    "    DATA = \"text\"\n",
    "    LABELS= \"intent\"\n",
    "\n",
    "    def __init__(self, train, test,  classes, tokenizer:FullTokenizer, max_sequence_length=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = 0\n",
    "        self.classes = classes\n",
    "\n",
    "        # train, test = map(lambda df: df.reindex(df[IntentDetection.DATA].str.len().sort_values().index), [train, test])\n",
    "\n",
    "        ((self.train_x, self.train_y), (self.test_x, self.text_y)) = map(self._prepare, [train, test])\n",
    "        self.max_sequence_length = min(self.max_sequence_length, max_sequence_length)\n",
    "        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "\n",
    "    def _prepare(self, data_frame):\n",
    "        x, y = [], []\n",
    "        for _, row in tqdm.tqdm(data_frame.iterrows()):\n",
    "            text, label = row[IntentDetection.DATA], row[IntentDetection.LABELS]\n",
    "\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            self.max_sequence_length = max(self.max_sequence_length, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x = []\n",
    "\n",
    "        for input_ids in ids:\n",
    "            # cut_off = min(len(input_ids), self.max_sequence_length-2)\n",
    "            cut_off = min(len(input_ids), self.max_sequence_length)\n",
    "            input_ids[:cut_off] \n",
    "            input_ids = input_ids + [0]*(self.max_sequence_length-len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "        \n",
    "        return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=\"bert_en_uncased_L-12_H-768_A-12_2/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'going', 'to', 'do']"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "tokenizer.tokenize(\"what is this going to do\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = tf.saved_model.load(\"./bert_en_uncased_L-12_H-768_A-12_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_length, bert_model):\n",
    "    input_layer = keras.layers.Input(shape=(max_sequence_length, ), dtype=tf.int32, name=\"input_layer\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    bert_layer = hub.KerasLayer(bert_model, trainable=True)\n",
    "    \n",
    "    pooled, seq = bert_layer([input_layer, input_mask, segment_ids])\n",
    "    print(pooled)\n",
    "    print(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "13784it [00:02, 5818.10it/s]\n",
      "700it [00:00, 5732.47it/s]\n"
     ]
    }
   ],
   "source": [
    "classes = train.intent.unique().tolist()\n",
    "data = IntentDetection(train, test, classes, tokenizer,  max_sequence_length=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor(\"keras_layer_17/Identity:0\", shape=(None, 768), dtype=float32)\nTensor(\"keras_layer_17/Identity_1:0\", shape=(None, 30, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_sequence_length, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}