{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "c5161659e0c4ba7248a1564d775f97f6b0cb1faea6f43745a651b61be4b2f423"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import bert\n",
    "import tqdm\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# https://www.youtube.com/watch?v=gE-95nFF4Cc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 1OlcvGWReJMuyYQuOZm149vHWwPtlboR6 --output intent/train.csv\n",
    "# !gdown --id 1Oi5cRlTybuIF2Fl5Bfsr-KkqrXrdt77w --output intent/valid.csv\n",
    "# !gdown --id 1ep9H6-HvhB4utJRLVcLzieWNUSG3P_uF --output intent/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"intent/train.csv\")\n",
    "validation = pd.read_csv(\"intent/valid.csv\")\n",
    "test = pd.read_csv(\"intent/test.csv\")\n",
    "\n",
    "train=train.append(validation).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = sns.countplot(train.intent, palette=HAPPY_COLORS_PALETTE)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=30, horizontalalignment=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class handles the tokenizing and padding of our dataset. Each dataset will have a slightly different way to do each step. On regular\n",
    "# TF packages, https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/2Cyzs/padding, you can see that they have methods that # tokenize and pad together that work seamlessly. Because we are using a pretrained model, bert-tf2, these are not available to us (not sure why tbh, probably because we want to use the pretrained tokenizer which requires us to not use TF methods)\n",
    "class IntentDetection:\n",
    "    DATA = \"text\"\n",
    "    LABELS = \"intent\"\n",
    "\n",
    "    def __init__(self, train, test,  classes, tokenizer:FullTokenizer, max_sequence_length=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sequence_length = 0\n",
    "        self.classes = classes\n",
    "\n",
    "        # train, test = map(lambda df: df.reindex(df[IntentDetection.DATA].str.len().sort_values().index), [train, test])\n",
    "\n",
    "        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "        self.max_sequence_length = min(self.max_sequence_length, max_sequence_length)\n",
    "        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "\n",
    "    def _prepare(self, data_frame):\n",
    "        x, y = [], []\n",
    "        for _, row in tqdm.tqdm(data_frame.iterrows()):\n",
    "            text, label = row[IntentDetection.DATA], row[IntentDetection.LABELS]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            self.max_sequence_length = max(self.max_sequence_length, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x = []\n",
    "\n",
    "        for input_ids in ids:\n",
    "            # cut_off = min(len(input_ids), self.max_sequence_length-2)\n",
    "            cut_off = min(len(input_ids), self.max_sequence_length)\n",
    "            input_ids[:cut_off] \n",
    "            input_ids = input_ids + [0]*(self.max_sequence_length-len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "        \n",
    "        return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\n",
    "tokenizer = FullTokenizer(vocab_file=\"bert_en_uncased_L-12_H-768_A-12_2/assets/vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = tf.saved_model.load(\"./bert_en_uncased_L-12_H-768_A-12_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.intent.unique().tolist()\n",
    "data = IntentDetection(train, test, classes, tokenizer,  max_sequence_length=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.train_y)\n",
    "for x in data.train_y:\n",
    "    print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_length, bert_model):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    input_layer = keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_layer\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    bert_layer = hub.KerasLayer(bert_model, trainable=True)\n",
    "    pooled, seq = bert_layer([input_layer, input_mask, segment_ids])\n",
    "\n",
    "    # input_l = tf.keras.layers.Input(shape=pooled.shape)\n",
    "    # x = bert_layer()(input_l)\n",
    "\n",
    "    # x = keras.layers.Lambda(lambda seq: seq[:, 0, :])(seq)\n",
    "    # print(x.shape)\n",
    "    x = keras.layers.Layer(pooled.shape)(pooled)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = keras.layers.Dense(768, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(len(classes), activation=\"softmax\")(x)\n",
    "    model = keras.Model([input_layer, input_mask, segment_ids], x, name=\"bert_model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(data.max_sequence_length, bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_points = \"checkpoint/checkpoint.hb\"\n",
    "check_point_dir = os.path.dirname(check_points)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=check_point_dir, verbose=1, monitor=\"val_sparse_categorical_accuracy\", save_best_only=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"log_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data.train_x is tokenized BEFORE the model begins running. First we create the tokenizer, then we embed the tokenizer with\n",
    "# a giant corpus of words not related to our data. Then we use that tokenizer on our data to put all the sentences in a token-form.\n",
    "# The tokenized data is then used to train the model.\n",
    "mask = np.zeros(data.train_x.shape)\n",
    "seg = np.zeros(data.train_x.shape)\n",
    "history = model.fit(x=[data.train_x, mask, seg], y=data.train_y, verbose=1, validation_split=0.1, batch_size=16, shuffle=True,\n",
    "use_multiprocessing=True, workers=5, epochs=5, callbacks=[cp_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"model_path\"\n",
    "os.makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = \"saved_model\"\n",
    "model_path = os.path.join(model_folder, saved_model)\n",
    "# tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "431/431 [==============================] - 17s 40ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9978\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.0529 - sparse_categorical_accuracy: 0.9829\n",
      "train acc 0.997750997543335\n",
      "train acc 0.9828571677207947\n"
     ]
    }
   ],
   "source": [
    "mask = np.zeros(data.train_x.shape)\n",
    "seg = np.zeros(data.train_x.shape)\n",
    "tmask = np.zeros(data.test_x.shape)\n",
    "tseg = np.zeros(data.test_x.shape)\n",
    "\n",
    "train_loss, train_acc = model.evaluate([data.train_x, mask, seg], data.train_y, batch_size=32)\n",
    "test_loss, test_acc = model.evaluate([data.test_x, tmask, tseg], data.test_y, batch_size=32)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\"train acc\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  101  3531  5587 11173 17183  4765  2000  2026  2377  9863  2023  2003\n 19166   102     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1031,\n",
       " 18856,\n",
       " 2015,\n",
       " 1033,\n",
       " 4952,\n",
       " 2000,\n",
       " 2225,\n",
       " 3676,\n",
       " 2213,\n",
       " 2632,\n",
       " 25438,\n",
       " 27395,\n",
       " 2006,\n",
       " 8224,\n",
       " 2189]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "print(data.train_x[4])\n",
    "tok = tokenizer.tokenize(\"[CLS] listen to westbam alumb allergic on google music\")\n",
    "tokenizer.convert_tokens_to_ids(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1031, 18856, 2015, 1033, 8224, 2377, 2924, 4070]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "tok = tokenizer.tokenize(\"[CLS] google play Bank account\")\n",
    "x = tokenizer.convert_tokens_to_ids(tok)\n",
    "print(x)\n",
    "np_x = np.array(x)\n",
    "np_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['PlayMusic', 'AddToPlaylist', 'RateBook', 'SearchScreeningEvent', 'BookRestaurant', 'GetWeather', 'SearchCreativeWork']\n",
      "[[4.5380e-06 5.1565e-06 1.6243e-05 1.2455e-05 9.4866e-06 9.9992e-01\n",
      "  2.7915e-05]\n",
      " [8.5619e-05 5.0605e-04 9.9662e-01 8.2558e-05 2.8746e-04 3.8173e-04\n",
      "  2.0361e-03]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "  \"how hot is it outside\",\n",
    "  \"Rate this book as awful\"\n",
    "]\n",
    "print(classes)\n",
    "%precision 4\n",
    "\n",
    "pred_tokens = map(tokenizer.tokenize, sentences)\n",
    "pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "\n",
    "pred_token_ids = map(lambda tids: tids +[0]*(data.max_sequence_length-len(tids)),pred_token_ids)\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "x1, x2 = np.zeros(pred_token_ids.shape), np.zeros(pred_token_ids.shape)\n",
    "\n",
    "predictions = model.predict([pred_token_ids, x1, x2])\n",
    "print(predictions)\n",
    "# for text, label in zip(sentences, predictions):\n",
    "#   print(\"text:\", text, \"\\nintent:\", classes[label])\n",
    "#   print()\n",
    "\n",
    "# output = model.predict([data.test_x, tmask, tseg])\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}